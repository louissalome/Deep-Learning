{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL. Task 3. LSTM\n",
    "\n",
    "<h5 style = \"text-align : right;\">Louis Salomé</h5>\n",
    "\n",
    "<br>Here are the main parts of this notebook :\n",
    "1. Pick poems\n",
    "3. Train LSTM as language model on your data (with 1 or 2 layers)\n",
    "4. Train GRU (with 1 or 2 layers)\n",
    "5. Compare metrics of GRU and LSTM. Compare poetries, generated by your models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "### import part\n",
    "\n",
    "# Process data\n",
    "import numpy as np\n",
    "\n",
    "# Process strings\n",
    "import re\n",
    "\n",
    "# Measure time\n",
    "import time\n",
    "from time import time\n",
    "\n",
    "#Simulate randomness\n",
    "import random as rd\n",
    "\n",
    "# Load Data\n",
    "import keras\n",
    "\n",
    "# Neural Networks\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Input,  Embedding, LSTM, GRU\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "import keras.utils as ku \n",
    "\n",
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "# Use only one of the 4 GPUs\n",
    "# Using the command nvidia-sim we can chose the freest GPU\n",
    "import sys\n",
    "import os \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1' # Here is the NUMBER_OF_GPU\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Take a poem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"\"\"Two roads diverged in a yellow wood,\n",
    "And sorry I could not travel both\n",
    "And be one traveler, long I stood\n",
    "And looked down one as far as I could\n",
    "To where it bent in the undergrowth;\n",
    "Then took the other, as just as fair,\n",
    "And having perhaps the better claim,\n",
    "Because it was grassy and wanted wear;\n",
    "Though as for that the passing there\n",
    "Had worn them really about the same,\n",
    "And both that morning equally lay\n",
    "In leaves no step had trodden black.\n",
    "Oh, I kept the first for another day!\n",
    "Yet knowing how way leads on to way,\n",
    "I doubted if I should ever come back.\n",
    "I shall be telling this with a sigh\n",
    "Somewhere ages and ages hence:\n",
    "Two roads diverged in a wood, and I—\n",
    "I took the one less traveled by,\n",
    "And that has made all the difference.\n",
    "I have been one acquainted with the night.\n",
    "I have walked out in rain—and back in rain.\n",
    "I have outwalked the furthest city light.\n",
    "I have looked down the saddest city lane.\n",
    "I have passed by the watchman on his beat\n",
    "And dropped my eyes, unwilling to explain.\n",
    "I have stood still and stopped the sound of feet\n",
    "When far away an interrupted cry\n",
    "Came over houses from another street,\n",
    "But not to call me back or say good-bye;\n",
    "And further still at an unearthly height,\n",
    "One luminary clock against the sky\n",
    "Proclaimed the time was neither wrong nor right. \n",
    "I have been one acquainted with the night.\n",
    "My long two-pointed ladder's sticking through a tree \n",
    "Toward heaven still, \n",
    "And there's a barrel that I didn't fill \n",
    "Beside it, and there may be two or three \n",
    "Apples I didn't pick upon some bough. \n",
    "But I am done with apple-picking now. \n",
    "Essence of winter sleep is on the night, \n",
    "The scent of apples: I am drowsing off. \n",
    "I cannot rub the strangeness from my sight \n",
    "I got from looking through a pane of glass \n",
    "I skimmed this morning from the drinking trough \n",
    "And held against the world of hoary grass. \n",
    "It melted, and I let it fall and break. \n",
    "But I was well \n",
    "Upon my way to sleep before it fell, \n",
    "And I could tell \n",
    "What form my dreaming was about to take. \n",
    "Magnified apples appear and disappear, \n",
    "Stem end and blossom end, \n",
    "And every fleck of russet showing clear. \n",
    "My instep arch not only keeps the ache, \n",
    "It keeps the pressure of a ladder-round. \n",
    "I feel the ladder sway as the boughs bend. \n",
    "And I keep hearing from the cellar bin \n",
    "The rumbling sound \n",
    "Of load on load of apples coming in. \n",
    "For I have had too much \n",
    "Of apple-picking: I am overtired \n",
    "Of the great harvest I myself desired. \n",
    "There were ten thousand thousand fruit to touch, \n",
    "Cherish in hand, lift down, and not let fall. \n",
    "For all \n",
    "That struck the earth, \n",
    "No matter if not bruised or spiked with stubble, \n",
    "Went surely to the cider-apple heap \n",
    "As of no worth. \n",
    "One can see what will trouble \n",
    "This sleep of mine, whatever sleep it is. \n",
    "Were he not gone, \n",
    "The woodchuck could say whether it's like his \n",
    "Long sleep, as I describe its coming on, \n",
    "Or just some human sleep.\n",
    "The way a crow \n",
    "Shook down on me \n",
    "The dust of snow \n",
    "From a hemlock tree \n",
    "Has given my heart \n",
    "A change of mood \n",
    "And saved some part \n",
    "Of a day I had rued.\n",
    "This saying good-bye on the edge of the dark\n",
    "And cold to an orchard so young in the bark\n",
    "Reminds me of all that can happen to harm\n",
    "An orchard away at the end of the farm\n",
    "All winter, cut off by a hill from the house.\n",
    "I don't want it girdled by rabbit and mouse,\n",
    "I don't want it dreamily nibbled for browse\n",
    "By deer, and I don't want it budded by grouse.\n",
    "(If certain it wouldn't be idle to call\n",
    "I'd summon grouse, rabbit, and deer to the wall\n",
    "And warn them away with a stick for a gun.)\n",
    "I don't want it stirred by the heat of the sun.\n",
    "(We made it secure against being, I hope,\n",
    "By setting it out on a northerly slope.)\n",
    "No orchard's the worse for the wintriest storm;\n",
    "But one thing about it, it mustn't get warm.\n",
    "\"How often already you've had to be told,\n",
    "Keep cold, young orchard. Good-bye and keep cold.\n",
    "Dread fifty above more than fifty below.\"\n",
    "I have to be gone for a season or so.\n",
    "My business awhile is with different trees,\n",
    "Less carefully nourished, less fruitful than these,\n",
    "And such as is done to their wood with an axe—\n",
    "Maples and birches and tamaracks.\n",
    "I wish I could promise to lie in the night\n",
    "And think of an orchard's arboreal plight\n",
    "When slowly (and nobody comes with a light)\n",
    "Its heart sinks lower under the sod.\n",
    "But something has to be left to God.\n",
    "The land was ours before we were the land’s.\n",
    "She was our land more than a hundred years\n",
    "Before we were her people. She was ours\n",
    "In Massachusetts, in Virginia,\n",
    "But we were England’s, still colonials,\n",
    "Possessing what we still were unpossessed by,\n",
    "Possessed by what we now no more possessed.\n",
    "Something we were withholding made us weak\n",
    "Until we found out that it was ourselves\n",
    "We were withholding from our land of living,\n",
    "And forthwith found salvation in surrender.\n",
    "Such as we were we gave ourselves outright\n",
    "(The deed of gift was many deeds of war)\n",
    "To the land vaguely realizing westward,\n",
    "But still unstoried, artless, unenhanced,\n",
    "Such as she was, such as she would become.\n",
    "Others taunt me with having knelt at well-curbs\n",
    "Always wrong to the light, so never seeing\n",
    "Deeper down in the well than where the water\n",
    "Gives me back in a shining surface picture\n",
    "Me myself in the summer heaven godlike\n",
    "Looking out of a wreath of fern and cloud puffs.\n",
    "Once, when trying with chin against a well-curb,\n",
    "I discerned, as I thought, beyond the picture,\n",
    "Through the picture, a something white, uncertain,\n",
    "Something more of the depths—and then I lost it.\n",
    "Water came to rebuke the too clear water.\n",
    "One drop fell from a fern, and lo, a ripple\n",
    "Shook whatever it was lay there at bottom,\n",
    "Blurred it, blotted it out. What was that whiteness?\n",
    "Truth? A pebble of quartz? For once, then, something.\n",
    "\"\"\"\n",
    "data = re.sub(r'[^\\w\\s]','',data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. LSTM\n",
    "\n",
    "Code from : https://medium.com/@shivambansal36/language-modelling-text-generation-using-lstms-deep-learning-for-nlp-ed36b224b275"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "def dataset_preparation(data):\n",
    "    # get tokens\n",
    "    corpus = data.lower().split(\"\\n\")    \n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "    print(\"Total number of words : \",total_words)\n",
    "    \n",
    "    # convert corpus into a flat dataset\n",
    "    input_sequences = []\n",
    "    for line in corpus:\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "    print(\"len(input_sequences)  : \",len(input_sequences))\n",
    "            \n",
    "    # uniformize data\n",
    "    max_sequence_len = max([len(x) for x in input_sequences])\n",
    "    input_sequences = np.array(pad_sequences(input_sequences,   \n",
    "                          maxlen=max_sequence_len, padding='pre'))\n",
    "    \n",
    "    \"\"\"\n",
    "    Sentence: \"they are learning data science\"\n",
    "    PREDICTORS             | LABEL\n",
    "    they                   | are\n",
    "    they are               | learning\n",
    "    they are learning      | data\n",
    "    they are learning data | science\n",
    "    \"\"\"\n",
    "    \n",
    "    # get labels\n",
    "    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "    label = ku.to_categorical(label, num_classes=total_words)\n",
    "    return predictors, label, max_sequence_len ,total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build our recurrent neural network\n",
    "def create_model(predictors, label, max_sequence_len, total_words, myLayer, twoLayers=False):\n",
    "    input_len = max_sequence_len - 1\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(total_words, 10, input_length=input_len))\n",
    "    if not(twoLayers):\n",
    "        model.add(myLayer(units = 150))\n",
    "    else :\n",
    "        model.add(myLayer(units = 150, return_sequences = True))\n",
    "        model.add(myLayer(units = 150))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(total_words, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    model.fit(predictors, label, epochs=100, verbose=0)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use rnn to generate text\n",
    "def generate_text(seed_text, next_words, max_sequence_len, model):\n",
    "    for j in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen= \n",
    "                             max_sequence_len-1, padding='pre')\n",
    "        predicted = model.predict_classes(token_list, verbose=0)\n",
    "  \n",
    "        output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \" + output_word\n",
    "    return seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words :  468\n",
      "len(input_sequences)  :  916\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "X, Y, max_len, total_words = dataset_preparation(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time needed to create models :  116.82\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "myLayer = LSTM\n",
    "start = time()\n",
    "modelLSTM        = create_model(X, Y, max_len, total_words, myLayer)\n",
    "modelLSTM2layers = create_model(X, Y, max_len, total_words, myLayer, twoLayers = True)\n",
    "print(\"time needed to create models : \",round(time()-start,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time elapsed for creating model :  100.77\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "myLayer = GRU\n",
    "start = time()\n",
    "modelGRU        = create_model(X, Y, max_len, total_words,myLayer)\n",
    "modelGRU2layers = create_model(X, Y, max_len, total_words,myLayer,twoLayers=True)\n",
    "print(\"time elapsed for creating model : \",round(time()-start,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Generate poetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary\n",
    "def create_dict():\n",
    "    corpus = data.lower().split(\"\\n\")\n",
    "    words = []\n",
    "    for i in range(len(corpus)) :\n",
    "        words += corpus[i].split(' ')\n",
    "    return words\n",
    "\n",
    "#create a new beginning of line\n",
    "def new_beginning(words):\n",
    "    l = len(words)\n",
    "    ind = rd.randint(0,l-2)\n",
    "    new = ''\n",
    "    new += words[ind]\n",
    "    new += ' '\n",
    "    new += words[ind+1]\n",
    "    new = new[0].upper() + new[1:].lower()\n",
    "    return new\n",
    "\n",
    "# generate a line\n",
    "def lets_test(str,model):\n",
    "    return generate_text(str, 4, max_len, model)\n",
    "\n",
    "# generate a poem\n",
    "def my_poems(model1, model2):\n",
    "    words = create_dict()\n",
    "    poem1, poem2 = '', ''\n",
    "    for i in range(8):\n",
    "        beginning = new_beginning(words)\n",
    "        poem1 += lets_test(beginning, model1)\n",
    "        poem2 += lets_test(beginning, model2)\n",
    "        poem1 += '\\n'\n",
    "        poem2 += '\\n'\n",
    "    return poem1, poem2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two following poems are generated line per line. We give the algorithm 2 words to begin each line, and it generates the end of the lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poem generated with model1 :\n",
      "\n",
      "Essence of winter sleep is on\n",
      "Sigh somewhere i am overtired bruised\n",
      "A light to the ciderapple heap\n",
      "By and birches and tamaracks it\n",
      "Grouse if been one acquainted with\n",
      "In the nourished the better claim\n",
      "Whatever it was grassy and wanted\n",
      "Houses from the pressure of a\n",
      "\n",
      "****************************\n",
      "\n",
      "Poem generated with model2 :\n",
      "\n",
      "Essence of winter sleep is on\n",
      "Sigh somewhere day and should ever\n",
      "A light of mood myself a\n",
      "By and birches and tamaracks let\n",
      "Grouse if diverged in a wood\n",
      "In the great harvest i myself\n",
      "Whatever it was lay there at\n",
      "Houses from were englands still colonials\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run this cell over and over to see new poems\n",
    "\n",
    "# pick 2 models from\n",
    "models = [modelLSTM, modelLSTM2layers, modelGRU, modelGRU2layers]\n",
    "\n",
    "model1, model2 = models[1],  models[3]\n",
    "poemLSTM, poemGRU = my_poems(model1, model2)\n",
    "\n",
    "print(\"Poem generated with model1 :\\n\\n\"+poemLSTM)\n",
    "print(\"****************************\\n\")\n",
    "print(\"Poem generated with model2 :\\n\\n\"+poemGRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's hard to compare the results logically. But still these algorithms won't write a bestseller tomorrow.\n",
    "We notice that GRU trains faster than LSTM."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
