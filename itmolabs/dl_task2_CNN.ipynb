{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html>\n",
    "<h1 style = \"text-align : center; border : 1px black solid; padding : 20px;\"> DL. Task 2. CNN</h1>\n",
    "\n",
    "<h5 style = \"text-align : right;\">Louis Salom√©</h5>\n",
    "\n",
    "<br>Here are the main parts of this notebook :\n",
    "1. Load Fashion MNIST from keras.datasets\n",
    "2. Train MLP and Random Forest on Fashion MNIST. For MLP use parameters search via grid search or random search\n",
    "3. Train CNN on Fashion MNIST with parameters search\n",
    "4. Train CNN autoencoder on Fashion MNIST with parameters search\n",
    "5. Train MLP and Random Forest on the latent vector from TRAINED CNN autoencoder\n",
    "6. Compare all results using plot/plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "### import part\n",
    "\n",
    "# Process data\n",
    "import numpy as np\n",
    "\n",
    "# Plot graphics\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from pprint import pprint\n",
    "\n",
    "# Measure time\n",
    "import time\n",
    "from time import time\n",
    "\n",
    "#Simulate randomness\n",
    "import random as rd\n",
    "\n",
    "# Load Data\n",
    "import keras\n",
    "from keras.datasets.fashion_mnist import load_data\n",
    "\n",
    "# Read Images\n",
    "import skimage\n",
    "from skimage.io import imread\n",
    "\n",
    "# Model selection\n",
    "import sklearn\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Neural Networks\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Input, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from keras import backend as K\n",
    "\n",
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "# grid search optimization\n",
    "import hyperopt\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import sys\n",
    "\n",
    "# Use only one of the 4 GPUs\n",
    "# Using the command nvidia-sim we can chose the freest GPU\n",
    "import os \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1' # Here is the NUMBER_OF_GPU\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. The Fashion MNIST Dataset\n",
    "\n",
    "This is a MNIST-like dataset from keras of 70,000 28x28 labeled fashion images. \n",
    "Its size is 69MB. \n",
    "It is labeled with 10 classes (from 0 to 9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "(x_train, y_train), (x_test, y_test) = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = x_train.shape[0]\n",
    "x_train = x_train[:size]\n",
    "y_train = y_train[:size]\n",
    "x_test = x_test[:int(size/2)]\n",
    "y_test = y_test[:int(size/2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 784) \n",
      "x_test shape : (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "# get its shape\n",
    "input_shape = x_train.shape\n",
    "[m    ,h,w] = x_train.shape\n",
    "[mtest,h,w] = x_test.shape\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "x_train = keras.utils.normalize(x_train,axis=1)\n",
    "x_test = keras.utils.normalize(x_test,axis=1)\n",
    "\n",
    "x_train = np.reshape(x_train, (m, h*w)) \n",
    "x_test = np.reshape(x_test, (mtest, h*w))\n",
    "y_train = np.reshape(y_train, (m, 1))\n",
    "y_test = np.reshape(y_test, (mtest, 1))\n",
    "\n",
    "print(\"x_train shape:\", x_train.shape, \"\\nx_test shape :\", x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# how does a sample look like ?\n",
    "r = rd.randint(1,m)\n",
    "plt.imshow(x_train[r].reshape((h,w)),cmap='gray')\n",
    "plt.title(\"x_train[\"+str(r)+']')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. MLP & RF and Tuning Hyperparameters\n",
    "\n",
    "In this part we try two different classifiers : the RandomForestClassifier from sklearn.ensemble and a Multilayer Perceptron with our own architecure.\n",
    "\n",
    "For both of them we will first try to fit a base_model, then do a random search to find quite good parameters, and then do a grid_search to fine better ones. Code inspired from https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n",
    "\n",
    "## 2.1 RandomForestClassifier\n",
    "\n",
    "### 2.1.1 A small forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time for fitting :  9.97 secondes.\n",
      "We found an accuracy of 85.1 %.\n"
     ]
    }
   ],
   "source": [
    "# initialize\n",
    "rf_base_model = RandomForestClassifier()\n",
    "\n",
    "# fit\n",
    "start=time()\n",
    "rf_base_model.fit(x_train, y_train.ravel())\n",
    "print(\"Elapsed time for fitting : \", round(time()-start,2),\"secondes.\")\n",
    "\n",
    "# predict\n",
    "y_predict = rf_base_model.predict(x_test)\n",
    "acc = accuracy_score(y_test.ravel(), y_predict, normalize=True)\n",
    "print(\"We found an accuracy of\",round(100*acc,2),\"%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Tuning hyperparameters of RandomForectClassifier by Random Search\n",
    "\n",
    "Random search allows us to narrow down the range for each hyperparameter. We will tune parameters such as :\n",
    "+ number of estimators\n",
    "+ maximum of features\n",
    "+ maximum depth\n",
    "+ minimum samples for a split\n",
    "+ minimume samples in a leaf\n",
    "+ use bootstrap ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the grid we're gonna use :\n",
      "{'bootstrap': [True, False],\n",
      " 'max_depth': [10, 27, 45, 62, 80, None],\n",
      " 'max_features': ['auto', 'sqrt'],\n",
      " 'min_samples_leaf': [1, 2, 4],\n",
      " 'min_samples_split': [2, 5, 10],\n",
      " 'n_estimators': [20, 88, 157, 225, 294, 362, 431, 500]}\n"
     ]
    }
   ],
   "source": [
    "# searching space\n",
    "n_estimators = [int(x) for x in np.linspace(start = 20, stop = 500, num = 8)]\n",
    "max_features = ['auto', 'sqrt']\n",
    "max_depth = [int(x) for x in np.linspace(10, 80, num = 5)]\n",
    "max_depth.append(None)\n",
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# grid\n",
    "random_grid = {\n",
    "    'n_estimators': n_estimators,\n",
    "    'max_features': max_features,\n",
    "    'max_depth': max_depth,\n",
    "    'min_samples_split': min_samples_split,\n",
    "    'min_samples_leaf': min_samples_leaf,\n",
    "    'bootstrap': bootstrap\n",
    "}\n",
    "print(\"This is the grid we're gonna use :\")\n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "[CV] max_depth=80, min_samples_leaf=4, n_estimators=362, max_features=sqrt, min_samples_split=5, bootstrap=True \n",
      "[CV]  max_depth=80, min_samples_leaf=4, n_estimators=362, max_features=sqrt, min_samples_split=5, bootstrap=True, total= 3.3min\n",
      "[CV] max_depth=80, min_samples_leaf=4, n_estimators=362, max_features=sqrt, min_samples_split=5, bootstrap=True \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  3.3min remaining:    0.0s\n"
     ]
    }
   ],
   "source": [
    "Niter = 5\n",
    "k_cv = 3\n",
    "\n",
    "start=time()\n",
    "# random search\n",
    "rf_random = RandomizedSearchCV(\n",
    "    estimator = RandomForestClassifier(), \n",
    "    param_distributions = random_grid, \n",
    "    n_iter = Niter, \n",
    "    cv = k_cv,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Fit the random search model\n",
    "rf_random.fit(x_train, y_train.ravel())\n",
    "\n",
    "# results\n",
    "print(\"Elapsed time for random search with\",Niter,\"iterations : \", round(time()-start,2),\"secondes.\")\n",
    "print(\"This is the best parameters we found :\")\n",
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 Grid Search\n",
    "\n",
    "Now that we know where to concentrate our search, we can explicitly specify every combination of settings to try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid\n",
    "param_grid = {\n",
    "    'bootstrap': [False],\n",
    "    'max_depth': [None],\n",
    "    'max_features': ['auto'],\n",
    "    'min_samples_leaf': [1,2],\n",
    "    'min_samples_split': [2, 3],\n",
    "    'n_estimators': [200,250,300]\n",
    "}\n",
    "\n",
    "pprint(param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many folds in the cross-validation ?\n",
    "k_cv = 3\n",
    "\n",
    "start=time()\n",
    "rf_grid = GridSearchCV(\n",
    "    estimator = RandomForestClassifier(), \n",
    "    param_grid = param_grid, \n",
    "    cv = k_cv,\n",
    "    verbose = 2\n",
    "    #n_jobs = -1, \n",
    ")\n",
    "\n",
    "# Fit the grid search to the data\n",
    "rf_grid.fit(x_train, y_train)\n",
    "\n",
    "print(\"Elapsed time for grid search : \", round(time()-start,2),\"secondes.\")\n",
    "rf_grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4 How efficient was the tuning ?\n",
    "\n",
    "Let's compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, x_test, y_test):\n",
    "    y_predict = model.predict(x_test)\n",
    "    accuracy = accuracy_score(y_test, y_predict, normalize=True)\n",
    "    print('Model Performance')\n",
    "    #print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n",
    "    print('Accuracy = {:0.3f}%.'.format(accuracy))\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf_base_model\n",
    "rf_base_accuracy = evaluate(rf_base_model, x_test, y_test)\n",
    "\n",
    "#rf_random_search\n",
    "rf_best_random = rf_random.best_estimator_\n",
    "random_accuracy = evaluate(best_random, x_test, y_test)\n",
    "print('Improvement between base and random of {:0.2f}%.'.format( 100 * (rf_random_accuracy - rf_base_accuracy) / base_accuracy))\n",
    "\n",
    "# rf_grid_search\n",
    "rf_best_grid = rf_grid.best_estimator_\n",
    "grid_accuracy = evaluate(best_grid, x_test, y_test)\n",
    "print('Improvement between grid and random of {:0.2f}%.'.format( 100 * (rf_grid_accuracy - rf_random_accuracy) / base_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 MLP\n",
    "\n",
    "### 2.2.1 A first MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize\n",
    "MLP_base_model = Sequential()\n",
    "MLP_base_model.add(Dense(64, activation=tf.nn.relu, input_dim=h*w))\n",
    "MLP_base_model.add(Dense(64, activation=tf.nn.relu))\n",
    "MLP_base_model.add(Dense(10, activation=tf.nn.softmax))\n",
    "\n",
    "# Compilation\n",
    "MLP_base_model.compile(\n",
    "              optimizer=keras.optimizers.Adam(lr=0.01),\n",
    "              loss='categorical_hinge',\n",
    "              metrics=['accuracy'])\n",
    "# model will be our mlp for the whole lab\n",
    "\n",
    "# Training\n",
    "start = time()\n",
    "MLP_base_model.fit(x_train, keras.utils.np_utils.to_categorical(y_train), epochs=20, batch_size=128, verbose = 0)\n",
    "print(\"Elapsed time for fitting : \", round(time()-start,2),\"secondes.\")\n",
    "\n",
    "# Predicting\n",
    "y_predict_onehot = MLP_base_model.predict([x_test])\n",
    "y_predict = np.zeros(mtest)\n",
    "for i in range(mtest):\n",
    "    y_predict[i] = np.argmax(y_predict_onehot[i])\n",
    "    \n",
    "# Calculating accuracy\n",
    "MLP_base_model_acc = accuracy_score(y_predict,y_test)\n",
    "print(\"We found an accuracy of\",round(100*MLP_base_model_acc,2),\"%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Tuning hyperparameters of MLP by Random Search\n",
    "\n",
    "Now we do the same as before with a neural network. This NN will be a MLP. \n",
    "We will try to tune some parameters such as :\n",
    "+ learning rate\n",
    "+ optimizer\n",
    "+ activation function\n",
    "+ depth of the network for MLP\n",
    "\n",
    "Code inspired from : https://github.com/keras-team/keras/issues/1591"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# searching space\n",
    "learning_rate = [0.004, 0.002, 0.001, 0.0005]\n",
    "optimizer = ['adadelta','adam','rmsprop']\n",
    "activation_function = [\"relu\",\"elu\",\"selu\"]\n",
    "activation_function_last = [\"softmax\"]\n",
    "depth = [2,3,4,5]\n",
    "epochs = [20,30]\n",
    "\n",
    "# grid\n",
    "random_grid = {\n",
    "    'learning_rate': hp.choice('learning_rate',learning_rate),\n",
    "    'optimizer':  hp.choice('optimizer',optimizer),\n",
    "    'activation_function': hp.choice('activation_function',activation_function),\n",
    "    'activation_function_last': hp.choice('activation_function_last',activation_function_last),\n",
    "    'depth': hp.choice('depth',depth),\n",
    "    'epochs': hp.choice('epochs',epochs)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to build a model for a given set of parameters\n",
    "def testMLP(params):\n",
    "    \n",
    "    # initialize\n",
    "    model = Sequential()\n",
    "    \n",
    "    # set depth\n",
    "    for i in range(params['depth'] -1):\n",
    "        model.add(Dense(64, \n",
    "                      activation=params['activation_function']) # set activation function\n",
    "               )\n",
    "    model.add(Dense(10, activation=params['activation_function_last']))\n",
    "    \n",
    "    # set optimizer\n",
    "    if params[\"optimizer\"]==\"adam\":\n",
    "        # default lr = 0.001\n",
    "        this_optimizer = keras.optimizers.Adam(lr=params['learning_rate'])\n",
    "    if params[\"optimizer\"]==\"adadelta\":\n",
    "        # default lr = 1\n",
    "        this_optimizer = keras.optimizers.Adadelta(lr=100*params['learning_rate'])\n",
    "    if params[\"optimizer\"]==\"rmsprop\":\n",
    "        # default lr = 0.002\n",
    "        this_optimizer = keras.optimizers.RMSprop(lr=2*params['learning_rate'])\n",
    "    \n",
    "    \n",
    "    # compile \n",
    "    model.compile(optimizer = this_optimizer,\n",
    "                  loss='categorical_hinge',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # fit\n",
    "    model.fit(x_train, \n",
    "              keras.utils.np_utils.to_categorical(y_train), \n",
    "              epochs=params['epochs'], \n",
    "              batch_size=128, \n",
    "              verbose = 0)\n",
    "    \n",
    "    # Predicting\n",
    "    y_predict_onehot = model.predict([x_test])\n",
    "    y_predict = np.zeros(mtest)\n",
    "    for i in range(mtest):\n",
    "        y_predict[i] = np.argmax(y_predict_onehot[i])\n",
    "\n",
    "    # Calculating accuracy\n",
    "    acc = accuracy_score(y_predict,y_test)\n",
    "\n",
    "    # display some informations\n",
    "    def printinfo():\n",
    "        print(\"*****************\")\n",
    "        print(\" learning rate            :\", params['learning_rate'],\"\\n\",\n",
    "              \"optimizer                :\", params['optimizer'],\"\\n\",\n",
    "              \"activation function      :\", params['activation_function'],\"\\n\",\n",
    "              \"activation function last :\", params['activation_function_last'],\"\\n\",\n",
    "              \"depth                    :\", params['depth'],\"\\n\",\n",
    "              \"epochs                   :\", params['epochs'])\n",
    "        print(\">>>> acc =\",round(100*acc,2),\"%.\\n\")\n",
    "        pass\n",
    "    printinfo()\n",
    "    \n",
    "    # return loss\n",
    "    return {'loss': -acc, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Niter = 20\n",
    "\n",
    "start = time()\n",
    "trials = Trials()\n",
    "best = fmin(testMLP, \n",
    "            random_grid, \n",
    "            algo=tpe.suggest, \n",
    "            max_evals=Niter, \n",
    "            trials=trials)\n",
    "print(\"Elapsed time for random search with\",Niter,\"iterations : \", round(time()-start,2),\"secondes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestparams_random = {\n",
    "    'learning_rate': learning_rate[best['learning_rate']],\n",
    "    'optimizer':  optimizer[best['optimizer']],\n",
    "    'activation_function': activation_function[best['activation_function']],\n",
    "    'depth': depth[best['depth']],\n",
    "    'epochs': epochs[best['epochs']]\n",
    "}\n",
    "pprint(bestparams_random)\n",
    "\n",
    "MLP_best_random_acc = -min(trials.losses())\n",
    "print(\"\\nBest accuracy found :\",MLP_best_random_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 How efficient was the tuning ?\n",
    "\n",
    "Let's compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Base model accuracy :\",round(100*MLP_base_model_acc,2),\"%.\")\n",
    "print(\"After tuning params :\",round(100*MLP_best_random_acc,2),\"%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. CNN\n",
    "\n",
    "So far, we used our images as 1-dimensional vectors. But some NN are designed to work efficiently on 2-D images. Let's retake our original data and use CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "(x_train, y_train), (x_test, y_test) = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = x_train.shape[0] #you can reduce the size of your dataset\n",
    "x_train = x_train[:size]\n",
    "y_train = y_train[:size]\n",
    "x_test = x_test[:int(size/2)]\n",
    "y_test = y_test[:int(size/2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get its shape\n",
    "[m    ,h,w] = x_train.shape\n",
    "[mtest,h,w] = x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "x_train = keras.utils.normalize(x_train,axis=1)\n",
    "x_test = keras.utils.normalize(x_test,axis=1)\n",
    "\n",
    "x_train = np.reshape(x_train, (m, h, w, 1)) \n",
    "x_test = np.reshape(x_test, (mtest, h, w, 1))\n",
    "y_train = np.reshape(y_train, (m, 1))\n",
    "y_test = np.reshape(y_test, (mtest, 1))\n",
    "\n",
    "print(\"x_train shape:\", x_train.shape, \"\\nx_test shape :\", x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 A first CNN\n",
    "\n",
    "Code inspired from http://adventuresinmachinelearning.com/keras-tutorial-cnn-11-lines/\n",
    "\n",
    "3. Train CNN on Fashion MNIST with parameters search\n",
    "4. Train CNN autoencoder on Fashion MNIST with parameters search\n",
    "5. Train MLP and Random Forest on the latent vector from TRAINED CNN autoencoder\n",
    "6. Compare all results using plot/plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize\n",
    "CNN_base_model = Sequential()\n",
    "CNN_base_model.add(Conv2D(32, kernel_size=(5, 5), strides=(1, 1),\n",
    "                 activation='relu',\n",
    "                 input_shape=x_train[0].shape))\n",
    "CNN_base_model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "CNN_base_model.add(Flatten())\n",
    "CNN_base_model.add(Dense(32, activation='relu'))\n",
    "CNN_base_model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compilation\n",
    "CNN_base_model.compile(\n",
    "              optimizer=keras.optimizers.Adam(lr=0.01),\n",
    "              loss='categorical_hinge',\n",
    "              metrics=['accuracy'])\n",
    "# model will be our mlp for the whole lab\n",
    "\n",
    "# Training\n",
    "start = time()\n",
    "CNN_base_model.fit(x_train, \n",
    "                   keras.utils.np_utils.to_categorical(y_train), \n",
    "                   epochs=20, \n",
    "                   batch_size=128, \n",
    "                   verbose = 0)\n",
    "print(\"Elapsed time for fitting : \", round(time()-start,2),\"secondes.\")\n",
    "\n",
    "# Predicting\n",
    "y_predict_onehot = CNN_base_model.predict([x_test])\n",
    "y_predict = np.zeros(mtest)\n",
    "for i in range(mtest):\n",
    "    y_predict[i] = np.argmax(y_predict_onehot[i])\n",
    "    \n",
    "# Calculating accuracy\n",
    "CNN_base_model_acc = accuracy_score(y_predict,y_test)\n",
    "print(\"We found an accuracy of\",round(100*CNN_base_model_acc,2),\"%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Tuning parameters of CNN\n",
    "\n",
    "In this part we will keep the good parameters we found for MLP.\n",
    "Now we will take a look at :\n",
    "+ kernel_size\n",
    "+ strides\n",
    "+ pool_size\n",
    "+ filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# searching space\n",
    "kernel_size = [(3,3),(4,4),(5,5),(6,6)]\n",
    "strides = [(1,1),(2,2),(3,3)]\n",
    "pool_size = [(2,2),(3,3)]\n",
    "\n",
    "# grid\n",
    "random_grid = {\n",
    "    'kernel_size': hp.choice('kernel_size',kernel_size),\n",
    "    'strides':  hp.choice('strides',strides),\n",
    "    'pool_size': hp.choice('pool_size',pool_size)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our grid. Let's do a function that test a given set of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to build a model for a given set of parameters\n",
    "\n",
    "def testCNN(params):\n",
    "    \n",
    "    # initialize\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(32,\n",
    "                     kernel_size=params['kernel_size'],\n",
    "                     strides=params['strides'],\n",
    "                     activation='relu',\n",
    "                     input_shape=x_train[0].shape))\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size=params['pool_size'] \n",
    "                            #,strides=params['strides']\n",
    "                          ))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "       \n",
    "    # compile \n",
    "    model.compile(optimizer = keras.optimizers.Adam(lr=0.001),\n",
    "                  loss='categorical_hinge',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # fit\n",
    "    model.fit(x_train, \n",
    "              keras.utils.np_utils.to_categorical(y_train), \n",
    "              epochs=20, \n",
    "              batch_size=128, \n",
    "              verbose = 0)\n",
    "    \n",
    "    # Predicting\n",
    "    y_predict_onehot = model.predict([x_test])\n",
    "    y_predict = np.zeros(mtest)\n",
    "    for i in range(mtest):\n",
    "        y_predict[i] = np.argmax(y_predict_onehot[i])\n",
    "\n",
    "                  \n",
    "    # Calculating accuracy\n",
    "    acc = accuracy_score(y_predict,y_test)\n",
    "\n",
    "                  \n",
    "    # display some informations\n",
    "    def printinfo():\n",
    "        print(\"*****************\")\n",
    "        print(\" kernel size :\", params['kernel_size'],\"\\n\",\n",
    "              \"strides      :\", params['strides'],\"\\n\",\n",
    "              \"pool size    :\", params['pool_size'],\"\\n\")\n",
    "        print(\">>>> acc =\",round(100*acc,2),\"%.\\n\")\n",
    "        pass\n",
    "    printinfo()\n",
    "    \n",
    "    # return loss\n",
    "    return {'loss': -acc, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can randomly try some points of our grid. We do it $N_{iter}$ times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Niter = 20\n",
    "\n",
    "start = time()\n",
    "trials = Trials()\n",
    "best = fmin(testCNN, \n",
    "            random_grid, \n",
    "            algo=tpe.suggest, \n",
    "            max_evals=Niter, \n",
    "            trials=trials)\n",
    "print(\"Elapsed time for random search with\",Niter,\"iterations : \", round(time()-start,2),\"secondes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestparams_random = {\n",
    "    'kernel_size': kernel_size[best['kernel_size']],\n",
    "    'strides':  strides[best['strides']],\n",
    "    'pool_size': pool_size[best['pool_size']]\n",
    "}\n",
    "print(bestparams_random)\n",
    "\n",
    "CNN_best_random_acc = -min(trials.losses())\n",
    "print(\"Best accuracy found :\",CNN_best_random_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How efficient was the tuning ?\n",
    "\n",
    "Let's compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Base model accuracy :\",round(100*CNN_base_model_acc,2),\"%.\")\n",
    "print(\"After tuning params :\",round(100*CNN_best_random_acc,2),\"%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. CNN Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = Input(shape=(h, w, 1))  \n",
    "\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(encoded)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=time()\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=4,\n",
    "                batch_size=128,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test)\n",
    "               )\n",
    "print(\"Elapsed time for fitting :\", round(time()-start,2),\"secondes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_imgs = autoencoder.predict(x_test)\n",
    "\n",
    "n = 10\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(1,n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Train Classifiers on Latent Vectors & Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Model(input_img, encoded)\n",
    "encoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "x_train_encoded = encoder.predict(x_train)\n",
    "x_test_encoded = encoder.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(1,n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + n)\n",
    "    plt.imshow(x_test_encoded[i].reshape(32, 49))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_encoded = np.reshape(x_train_encoded, (m, 32*49)) \n",
    "x_test_encoded = np.reshape(x_test_encoded, (mtest, 32*49)) \n",
    "\n",
    "x_train = np.reshape(x_train, (m, h*w)) \n",
    "x_test = np.reshape(x_test, (mtest, h*w))\n",
    "\n",
    "y_train = np.reshape(y_train, (m,))\n",
    "y_test = np.reshape(y_test, (mtest,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the results with the previous ones.\n",
    "\n",
    "Best RF parameters :\n",
    "+ 'bootstrap': False,\n",
    "+ 'max_depth': None,\n",
    "+ 'max_features': 'auto',\n",
    "+ 'min_samples_leaf': 1,\n",
    "+ 'min_samples_split': 2,\n",
    "+ 'n_estimators': 260}\n",
    "\n",
    "Best MLP parameters :\n",
    "+ 'activation_function': 'relu',\n",
    "+ 'depth': 4,\n",
    "+ 'epochs': 30,\n",
    "+ 'learning_rate': 0.001,\n",
    "+ 'optimizer': 'adam'\n",
    "\n",
    "Best CNN parameters :\n",
    "+ 'kernel_size': (3, 3)\n",
    "+ 'strides': (1, 1)\n",
    "+ 'pool_size': (2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_best = RandomForestClassifier(n_estimators=260,bootstrap=False,max_depth=None,max_features='auto',min_samples_leaf=1,min_samples_split=2)\n",
    "\n",
    "# Orginal data\n",
    "rf_best.fit(x_train, y_train)\n",
    "y_predict = rf_best.predict(x_test)\n",
    "acc = accuracy_score(y_test, y_predict, normalize=True)\n",
    "print(\"Best RF on original data :\",round(100*acc,2),\"%.\")\n",
    "\n",
    "# Encoded data\n",
    "rf_best.fit(x_train_encoded, y_train)\n",
    "y_predict = rf_best.predict(x_test_encoded)\n",
    "acc = accuracy_score(y_test, y_predict, normalize=True)\n",
    "print(\"Best RF on Encoded data :\",round(100*acc,2),\"%.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original data\n",
    "MLP_best = Sequential()\n",
    "MLP_best.add(Dense(64, activation=tf.nn.relu, input_dim=h*w))\n",
    "MLP_best.add(Dense(64, activation=tf.nn.relu))\n",
    "MLP_best.add(Dense(64, activation=tf.nn.relu))\n",
    "MLP_best.add(Dense(64, activation=tf.nn.relu))\n",
    "MLP_best.add(Dense(10, activation=tf.nn.softmax))\n",
    "\n",
    "MLP_best.compile(\n",
    "              optimizer=keras.optimizers.Adam(lr=0.001),\n",
    "              loss='categorical_hinge',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "MLP_best.fit(x_train, keras.utils.np_utils.to_categorical(y_train), epochs=30, batch_size=128, verbose = 0)\n",
    "\n",
    "y_predict_onehot = MLP_best.predict([x_test])\n",
    "y_predict = np.zeros(mtest)\n",
    "for i in range(mtest):\n",
    "    y_predict[i] = np.argmax(y_predict_onehot[i])\n",
    "    \n",
    "    \n",
    "MLP_best_acc = accuracy_score(y_predict,y_test)\n",
    "print(\"Best MLP on original data :\",round(100*MLP_best_acc,2),\"%.\")\n",
    "\n",
    "\n",
    "\n",
    "# Encoded data\n",
    "MLP_best = Sequential()\n",
    "MLP_best.add(Dense(64, activation=tf.nn.relu, input_dim=32*49))\n",
    "MLP_best.add(Dense(64, activation=tf.nn.relu))\n",
    "MLP_best.add(Dense(64, activation=tf.nn.relu))\n",
    "MLP_best.add(Dense(64, activation=tf.nn.relu))\n",
    "MLP_best.add(Dense(10, activation=tf.nn.softmax))\n",
    "\n",
    "MLP_best.compile(\n",
    "              optimizer=keras.optimizers.Adam(lr=0.001),\n",
    "              loss='categorical_hinge',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "MLP_best.fit(x_train_encoded, keras.utils.np_utils.to_categorical(y_train), epochs=30, batch_size=128, verbose = 0)\n",
    "\n",
    "y_predict_onehot = MLP_best.predict([x_test_encoded])\n",
    "y_predict = np.zeros(mtest)\n",
    "for i in range(mtest):\n",
    "    y_predict[i] = np.argmax(y_predict_onehot[i])\n",
    "    \n",
    "MLP_best_acc = accuracy_score(y_predict,y_test)\n",
    "print(\"Best MLP on encoded data :\",round(100*MLP_best_acc,2),\"%.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.reshape(x_train, (m, h, w, 1)) \n",
    "x_test = np.reshape(x_test, (mtest, h, w, 1))\n",
    "\n",
    "x_train_encoded = np.reshape(x_train_encoded, (m, 32, 49, 1)) \n",
    "x_test_encoded = np.reshape(x_test_encoded, (mtest, 32, 49, 1))\n",
    "\n",
    "y_train = np.reshape(y_train, (m, 1))\n",
    "y_test = np.reshape(y_test, (mtest, 1))\n",
    "\n",
    "\n",
    "# Original data\n",
    "CNN_best = Sequential()\n",
    "CNN_best.add(Conv2D(32, kernel_size=(3, 3), strides=(1, 1),\n",
    "                 activation='relu',\n",
    "                 input_shape=x_train[0].shape))\n",
    "CNN_best.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "CNN_best.add(Flatten())\n",
    "CNN_best.add(Dense(32, activation='relu'))\n",
    "CNN_best.add(Dense(10, activation='softmax'))\n",
    "\n",
    "CNN_best.compile(\n",
    "              optimizer=keras.optimizers.Adam(lr=0.001),\n",
    "              loss='categorical_hinge',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "CNN_best.fit(x_train, \n",
    "               keras.utils.np_utils.to_categorical(y_train), \n",
    "               epochs=20, \n",
    "               batch_size=128, \n",
    "               verbose = 0)\n",
    "\n",
    "y_predict_onehot = CNN_best.predict([x_test])\n",
    "y_predict = np.zeros(mtest)\n",
    "for i in range(mtest):\n",
    "    y_predict[i] = np.argmax(y_predict_onehot[i])\n",
    "CNN_best_acc = accuracy_score(y_predict,y_test)\n",
    "print(\"Best CNN on original data\",round(100*CNN_best_acc,2),\"%.\")\n",
    "\n",
    "# Encoded data\n",
    "CNN_best = Sequential()\n",
    "CNN_best.add(Conv2D(32, kernel_size=(3, 3), strides=(1, 1),\n",
    "                 activation='relu',\n",
    "                 input_shape=x_train_encoded[0].shape))\n",
    "CNN_best.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "CNN_best.add(Flatten())\n",
    "CNN_best.add(Dense(32, activation='relu'))\n",
    "CNN_best.add(Dense(10, activation='softmax'))\n",
    "\n",
    "CNN_best.compile(\n",
    "              optimizer=keras.optimizers.Adam(lr=0.001),\n",
    "              loss='categorical_hinge',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "CNN_best.fit(x_train_encoded, \n",
    "               keras.utils.np_utils.to_categorical(y_train), \n",
    "               epochs=20, \n",
    "               batch_size=128, \n",
    "               verbose = 0)\n",
    "\n",
    "y_predict_onehot = CNN_best.predict([x_test_encoded])\n",
    "y_predict = np.zeros(mtest)\n",
    "for i in range(mtest):\n",
    "    y_predict[i] = np.argmax(y_predict_onehot[i])\n",
    "CNN_best_acc = accuracy_score(y_predict,y_test)\n",
    "print(\"Best CNN on encoded data\",round(100*CNN_best_acc,2),\"%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the results here are not relevant because the models trained on a too little amount of data, the random/grid searches were only performed a few time. But still the best score ever had sometimes have been obtained with CNN (more than 90% accuracy!). This doesn't come as a surprise as we know that CNN are designed for image recognition. We also seen at the end of the lab that the performance were almost the same on encoded data than on the original data. This can be a nice way to decrease the computation time if the images are too big and can be easily compressed. But we also see that a MaxPool or a Conv2D is a nonsense on such an image because there are no informaitons in shapes in such images. MLP & RandomForest are good basic classifiers for almost any problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
